{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rubencito/micromamba/envs/colorization/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/rubencito/micromamba/envs/colorization/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <103AD477-408D-3C2F-84B8-B7B8FFF88455> /Users/rubencito/micromamba/envs/colorization/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <D1E1EC61-A5AA-3BBA-94B8-043C751C8A50> /Users/rubencito/micromamba/envs/colorization/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "2024-07-24 22:39:26.680489: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# %config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "from glob import glob\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from skimage import util\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "# import albumentations as A\n",
    "# from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2, functional\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import Adam, rmsprop\n",
    "from warnings import warn\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow import data as tf_data\n",
    "from tensorflow import image as tf_image\n",
    "from tensorflow import io as tf_io\n",
    "\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# Lorenz's libs\n",
    "# import math\n",
    "import pandas as pd\n",
    "import requests\n",
    "from io import BytesIO\n",
    "# from pyproj import Proj, Transformer\n",
    "import random\n",
    "# from tqdm import tqdm\n",
    "# import folium\n",
    "# from folium.plugins import MarkerCluster\n",
    "\n",
    "from toloboy.toloboy import RGB2LAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.2\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define helper functions/classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LTransformation(object):\n",
    "    def __init__(self, contrast_range=(0.9, 1), brightness_range=(-0.05, 0.20), noise_var_range=(0, 0.005)):\n",
    "        self.contrast_range = contrast_range\n",
    "        self.brightness_range = brightness_range\n",
    "        self.noise_var_range = noise_var_range\n",
    "\n",
    "    def _apply_factor(self, L_channel, contrast_factor, brightness_factor):\n",
    "        # Apply adjusted brightness and contrast to the L channel\n",
    "        L_adjusted = contrast_factor * L_channel + brightness_factor\n",
    "\n",
    "        # Clip adjusted L channel to [0, 1]\n",
    "        L_adjusted = np.clip(L_adjusted, 0, 1)\n",
    "\n",
    "        return L_adjusted\n",
    "\n",
    "    def _apply_noise(self, L_channel, noise_var):\n",
    "        # Apply Gaussian noise to the L channel\n",
    "        L_noisy = util.random_noise(L_channel, mode='gaussian', var=noise_var)\n",
    "\n",
    "        # Clip noisy L channel to [0, 1]\n",
    "        L_noisy = np.clip(L_noisy, 0, 1)\n",
    "\n",
    "        return L_noisy\n",
    "\n",
    "    def _randomize_factors(self):\n",
    "        return np.random.uniform(*self.brightness_range), np.random.uniform(*self.contrast_range)\n",
    "\n",
    "    def _randomize_noise_var(self):\n",
    "        return np.random.uniform(*self.noise_var_range)\n",
    "\n",
    "    def __call__(self, L_channel):\n",
    "        while True:\n",
    "            brightness_factor, contrast_factor = self._randomize_factors()\n",
    "            noise_var = self._randomize_noise_var()\n",
    "\n",
    "            # Apply adjusted brightness and contrast to the L channel\n",
    "            L_adjusted = self._apply_factor(L_channel, contrast_factor, brightness_factor)\n",
    "\n",
    "            # Apply Gaussian noise to the L channel\n",
    "            L_augmented = self._apply_noise(L_adjusted, noise_var)\n",
    "\n",
    "            # Check if values are within range\n",
    "            if 0 <= np.min(L_augmented) <= np.max(L_augmented) <= 1:\n",
    "                break\n",
    "\n",
    "        return L_augmented, contrast_factor, brightness_factor, noise_var\n",
    "\n",
    "def convert_RGB_to_feed_model(img):\n",
    "    img = np.asarray(img)\n",
    "    sz_x = img.shape[0]\n",
    "    sz_y = img.shape[1]\n",
    "\n",
    "    train_imgs = np.zeros((sz_x, sz_y, 2))\n",
    "    train_input = np.zeros((sz_x, sz_y, 1))\n",
    "\n",
    "    R1 = np.reshape(img[:, :, 0], (sz_x * sz_y, 1))\n",
    "    G1 = np.reshape(img[:, :, 1], (sz_x * sz_y, 1))\n",
    "    B1 = np.reshape(img[:, :, 2], (sz_x * sz_y, 1))\n",
    "    L, A, B = RGB2LAB(R1, G1, B1)\n",
    "\n",
    "    train_input[:, :, 0] = L.reshape((sz_x, sz_y))\n",
    "    train_imgs[:, :, 0] = np.reshape(A, (sz_x, sz_y))\n",
    "    train_imgs[:, :, 1] = np.reshape(B, (sz_x, sz_y))\n",
    "\n",
    "    return train_input, train_imgs\n",
    "\n",
    "\n",
    "def convert_RGB__and_augment_to_feed_model(img):\n",
    "    img = np.asarray(img)\n",
    "    sz_x = img.shape[0]\n",
    "    sz_y = img.shape[1]\n",
    "\n",
    "    train_imgs = np.zeros((sz_x, sz_y, 2))\n",
    "    train_input = np.zeros((sz_x, sz_y, 1))\n",
    "\n",
    "    R1 = np.reshape(img[:, :, 0], (sz_x * sz_y, 1))\n",
    "    G1 = np.reshape(img[:, :, 1], (sz_x * sz_y, 1))\n",
    "    B1 = np.reshape(img[:, :, 2], (sz_x * sz_y, 1))\n",
    "    L, A, B = RGB2LAB(R1, G1, B1)\n",
    "\n",
    "    # Apply LTransformation to the L channel\n",
    "    L_transformation = LTransformation()\n",
    "    L_augmented, _, _, _ = L_transformation(L.reshape((sz_x, sz_y)))\n",
    "\n",
    "    train_input[:, :, 0] = L_augmented\n",
    "    train_imgs[:, :, 0] = np.reshape(A, (sz_x, sz_y))\n",
    "    train_imgs[:, :, 1] = np.reshape(B, (sz_x, sz_y))\n",
    "\n",
    "    return train_input, train_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define custom Dataset class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwisstopoDataset:\n",
    "    def __init__(self, img_indx, transform=None, large_dataset=False, return_label=True, batch_size=32, shuffle=False):\n",
    "        self.img_indx = img_indx\n",
    "        self.transform = transform\n",
    "        self.large_dataset = large_dataset\n",
    "        self.return_label = return_label\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        # Set the appropriate port based on the dataset size\n",
    "        self.port = 1986 if self.large_dataset else 1985\n",
    "\n",
    "        # Load metadata\n",
    "        self.metadata_file = self._load_metadata()\n",
    "\n",
    "    def _load_metadata(self):\n",
    "        raw_data_csv_file_link = f\"https://perritos.myasustor.com:{self.port}/metadata.csv\"\n",
    "        return pd.read_csv(raw_data_csv_file_link, index_col=0)\n",
    "\n",
    "    def _fetch_image(self, img_id):\n",
    "        img_in_server_link = f\"https://perritos.myasustor.com:{self.port}/data/img_id_{img_id}.jpg\"\n",
    "        response = requests.get(img_in_server_link)\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "        return image\n",
    "\n",
    "    def _process_image(self, img_id):\n",
    "        try:\n",
    "            image = self._fetch_image(img_id)\n",
    "        except Exception as e:\n",
    "            warn(f\"{'*'*5} Problem loading image id: {img_id} {'*'*5}\")\n",
    "            raise e\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = tf.keras.preprocessing.image.img_to_array(image)\n",
    "            image = image / 255.0  # Default normalization\n",
    "        return image\n",
    "\n",
    "    def _get_label(self, idx):\n",
    "        return self.metadata_file[\"class\"].iloc[idx]\n",
    "\n",
    "    def _generator(self):\n",
    "        if self.shuffle:\n",
    "            img_indices = np.random.permutation(len(self.img_indx))\n",
    "        else:\n",
    "            img_indices = self.img_indx\n",
    "\n",
    "        for idx in range(len(self.img_indx)):\n",
    "            image = self._process_image(self.img_indx[idx])\n",
    "            L, AB = image  # Unpack the transformed image\n",
    "            if self.return_label:\n",
    "                label = self._get_label(idx)\n",
    "                yield (L, AB), label\n",
    "            else:\n",
    "                yield L, AB\n",
    "\n",
    "    def get_dataset(self):\n",
    "        # Dynamically infer the shapes of L and AB channels\n",
    "        def _dynamic_output_signature():\n",
    "            example_image = self._fetch_image(self.img_indx[0])\n",
    "            example_transformed = self.transform(example_image)\n",
    "            L, AB = example_transformed\n",
    "            L_shape = tf.TensorSpec(shape=L.shape, dtype=tf.float32)\n",
    "            AB_shape = tf.TensorSpec(shape=AB.shape, dtype=tf.float32)\n",
    "            if self.return_label:\n",
    "                return ((L_shape, AB_shape), tf.TensorSpec(shape=(), dtype=tf.int64))\n",
    "            else:\n",
    "                return (L_shape, AB_shape)\n",
    "\n",
    "        output_signature = _dynamic_output_signature()\n",
    "\n",
    "        dataset = tf.data.Dataset.from_generator(self._generator, output_signature=output_signature)\n",
    "        dataset = dataset.batch(self.batch_size, drop_remainder=True) # use drop reminder to have same size always\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_LAB_transform(image):\n",
    "    L, AB = convert_RGB_to_feed_model(image)\n",
    "    return (L, AB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_LAB_and_augment_transform(image):\n",
    "    if np.random.rand() < 0.25:  # only apply augmentation to 25% of the data\n",
    "        L, AB = convert_RGB__and_augment_to_feed_model(image)\n",
    "    else:\n",
    "        L, AB = convert_RGB_to_feed_model(image)\n",
    "    return (L, AB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check info from the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was initially created using the scripts `retrieve_data.ipynb` and stored in a private server for later (re)use.\n",
    "In the metadata.csv file we get the information on original link, class and coordinates of each image.\n",
    "\n",
    "NOTE: the following are links stored in a private server, jet they are still publically available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 12960 entries, 0 to 12959\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   img_id      12960 non-null  int64  \n",
      " 1   img_name    12960 non-null  object \n",
      " 2   latitude    12960 non-null  float64\n",
      " 3   longitude   12960 non-null  float64\n",
      " 4   zoom_level  12960 non-null  int64  \n",
      " 5   class       12960 non-null  int64  \n",
      " 6   link        12960 non-null  object \n",
      "dtypes: float64(2), int64(3), object(2)\n",
      "memory usage: 810.0+ KB\n"
     ]
    }
   ],
   "source": [
    "is_large_dataset = True\n",
    "\n",
    "if is_large_dataset:\n",
    "    server_port = 1986 # Large dataset of ~10K images\n",
    "else:\n",
    "    server_port = 1985 # Large dataset of ~10K images\n",
    "# server_port = 1985 # initial dataset of 3.6K images\n",
    "\n",
    "raw_data_csv_file_link = f\"https://perritos.myasustor.com:{server_port}/metadata.csv\"\n",
    "\n",
    "\n",
    "metadata_raw_df = pd.read_csv(raw_data_csv_file_link, index_col=0)\n",
    "metadata_raw_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the Train, Valid and Test subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the column `image_id` from the metadata as index of the images and then we perform standard shufling and splitting.\n",
    "\n",
    "~~The final ratio for the train, validation and test dastasets are: 70, 29 and 1 % respectively~~\n",
    "\n",
    "The final ratio for the train, validation and test dastasets are: 59, 25 and 16 % respectively.\n",
    "\n",
    "NOTE: we add additonal ~3000 images for computing the FID score metrics, rusulting in roughly using 10K images for training/valiation (in a ratio of 70/30% repectively) and 3K images for testing. Total images used were 12960.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the size fo the train dataset is: 7602.\n",
      "the size fo the validation dataset is: 3258.\n",
      "the size fo the test dataset is: 2100.\n"
     ]
    }
   ],
   "source": [
    "dataX, dataY = metadata_raw_df[\"img_id\"].to_list(), metadata_raw_df[\"class\"] .to_list()\n",
    "\n",
    "rand_state = 9898\n",
    "\n",
    "train_ratio = 0.5866\n",
    "validation_ratio = 0.2514\n",
    "test_ratio = 0.162\n",
    "\n",
    "\n",
    "\n",
    "# train is now 75% of the entire data set\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataX, dataY, test_size=1 - train_ratio, stratify = dataY, random_state=rand_state)\n",
    "\n",
    "# test is now 10% of the initial data set\n",
    "# validation is now 15% of the initial data set\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio), stratify=y_test, random_state=rand_state)\n",
    "\n",
    "print(f\"the size fo the train dataset is: {len(x_train)}.\\nthe size fo the validation dataset is: {len(x_val)}.\\nthe size fo the test dataset is: {len(x_test)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_size =64\n",
    "\n",
    "# Instantiate the dataset\n",
    "# img_indices = [0, 1, 2, 3, 4, 5]  # Example indices\n",
    "\n",
    "train_dataset_loader = SwisstopoDataset(x_train,\n",
    "                           transform=convert_to_LAB_and_augment_transform,\n",
    "                           large_dataset=True,\n",
    "                           return_label=False,\n",
    "                           batch_size=b_size,\n",
    "                           shuffle=True)\n",
    "\n",
    "valid_dataset_loader = SwisstopoDataset(x_val,\n",
    "                           transform=convert_to_LAB_transform,\n",
    "                           large_dataset=True,\n",
    "                           return_label=False,\n",
    "                           batch_size=b_size,\n",
    "                           shuffle=False)\n",
    "\n",
    "test_dataset_loader = SwisstopoDataset(x_test,\n",
    "                           transform=convert_to_LAB_transform,\n",
    "                           large_dataset=True,\n",
    "                           return_label=False,\n",
    "                           batch_size=b_size,\n",
    "                           shuffle=False)\n",
    "\n",
    "train_dataset = train_dataset_loader.get_dataset()\n",
    "test_dataset = test_dataset_loader.get_dataset()\n",
    "valid_dataset = valid_dataset_loader.get_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tf.data.Dataset\n",
    "# Iterate over the dataset\n",
    "for batch in train_dataset:\n",
    "    # (L_channel, AB_channels), labels = batch # print with labels\n",
    "    # print(L_channel.shape, AB_channels.shape, print(labels.shape))\n",
    "    L_channel, AB_channels= batch # print without labels\n",
    "    print(L_channel.shape, AB_channels.shape)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
